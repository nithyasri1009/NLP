{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkroPqbW3rJHYwiuY9vepm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithyasri1009/NLP/blob/main/AutoTrans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Aql9KIi5tb5P",
        "outputId": "2108e530-796a-4c2e-9374-4bb82dfe7695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translators\n",
            "  Downloading translators-6.0.1-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from translators) (0.28.1)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from translators) (2.32.3)\n",
            "Collecting niquests>=3.14.0 (from translators)\n",
            "  Downloading niquests-3.14.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting exejs>=0.0.4 (from translators)\n",
            "  Downloading exejs-0.0.4-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=5.4.0 in /usr/local/lib/python3.11/dist-packages (from translators) (5.4.0)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.11/dist-packages (from translators) (4.67.1)\n",
            "Collecting pathos>=0.3.4 (from translators)\n",
            "  Downloading pathos-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cloudscraper>=1.2.71 (from translators)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: cryptography>=42.0.4 in /usr/local/lib/python3.11/dist-packages (from translators) (43.0.3)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.11/dist-packages (from cloudscraper>=1.2.71->translators) (3.2.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from cloudscraper>=1.2.71->translators) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=42.0.4->translators) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->translators) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->translators) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->translators) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->translators) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->translators) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from niquests>=3.14.0->translators) (3.4.2)\n",
            "Collecting urllib3-future<3,>=2.12.900 (from niquests>=3.14.0->translators)\n",
            "  Downloading urllib3_future-2.13.901-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting wassima<2,>=1.0.1 (from niquests>=3.14.0->translators)\n",
            "  Downloading wassima-1.2.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting ppft>=1.7.7 (from pathos>=0.3.4->translators)\n",
            "  Downloading ppft-1.7.7-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.4.0 (from pathos>=0.3.4->translators)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.6 (from pathos>=0.3.4->translators)\n",
            "  Downloading pox-0.3.6-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.18 (from pathos>=0.3.4->translators)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->translators) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=42.0.4->translators) (2.22)\n",
            "Collecting jh2<6.0.0,>=5.0.3 (from urllib3-future<3,>=2.12.900->niquests>=3.14.0->translators)\n",
            "  Downloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting qh3<2.0.0,>=1.5.3 (from urllib3-future<3,>=2.12.900->niquests>=3.14.0->translators)\n",
            "  Downloading qh3-1.5.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->translators) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->translators) (4.14.1)\n",
            "Downloading translators-6.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exejs-0.0.4-py3-none-any.whl (11 kB)\n",
            "Downloading niquests-3.14.1-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3_future-2.13.901-py3-none-any.whl (669 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.9/669.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wassima-1.2.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.9/247.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jh2-5.0.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qh3-1.5.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wassima, qh3, ppft, pox, jh2, exejs, dill, urllib3-future, multiprocess, pathos, niquests, cloudscraper, translators\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudscraper-1.2.71 dill-0.4.0 exejs-0.0.4 jh2-5.0.9 multiprocess-0.70.18 niquests-3.14.1 pathos-0.3.4 pox-0.3.6 ppft-1.7.7 qh3-1.5.3 translators-6.0.1 urllib3-future-2.13.901 wassima-1.2.2\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f21331e962cc7c520193360a5fa741f3530871093c720533fb84c1e35fe7628f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fb4a7eddfecea1db03.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fb4a7eddfecea1db03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "!pip install translators\n",
        "!pip install langdetect\n",
        "!pip install\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from typing import Optional\n",
        "import translators as ts\n",
        "from langdetect import detect\n",
        "\n",
        "# Supported language pairs for MarianMT\n",
        "SUPPORTED_PAIRS = {\n",
        "    # English to Others\n",
        "    \"en\": {\n",
        "        \"es\": \"Helsinki-NLP/opus-mt-en-es\",\n",
        "        \"fr\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
        "        \"de\": \"Helsinki-NLP/opus-mt-en-de\",\n",
        "        \"hi\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
        "        \"ru\": \"Helsinki-NLP/opus-mt-en-ru\",\n",
        "        \"zh\": \"Helsinki-NLP/opus-mt-en-zh\",\n",
        "        \"ja\": \"Helsinki-NLP/opus-mt-en-ja\",\n",
        "        \"ar\": \"Helsinki-NLP/opus-mt-en-ar\"\n",
        "    },\n",
        "    # Others to English\n",
        "    \"es\": {\"en\": \"Helsinki-NLP/opus-mt-es-en\"},\n",
        "    \"fr\": {\"en\": \"Helsinki-NLP/opus-mt-fr-en\"},\n",
        "    \"de\": {\"en\": \"Helsinki-NLP/opus-mt-de-en\"},\n",
        "    \"hi\": {\"en\": \"Helsinki-NLP/opus-mt-hi-en\"},\n",
        "    \"ru\": {\"en\": \"Helsinki-NLP/opus-mt-ru-en\"},\n",
        "    \"zh\": {\"en\": \"Helsinki-NLP/opus-mt-zh-en\"},\n",
        "    \"ja\": {\"en\": \"Helsinki-NLP/opus-mt-ja-en\"},\n",
        "    \"ar\": {\"en\": \"Helsinki-NLP/opus-mt-ar-en\"}\n",
        "}\n",
        "\n",
        "# Mapping between langdetect codes and our supported codes\n",
        "LANG_MAPPING = {\n",
        "    'en': 'en',\n",
        "    'es': 'es',\n",
        "    'fr': 'fr',\n",
        "    'de': 'de',\n",
        "    'hi': 'hi',\n",
        "    'ru': 'ru',\n",
        "    'zh-cn': 'zh',\n",
        "    'ja': 'ja',\n",
        "    'ar': 'ar'\n",
        "}\n",
        "\n",
        "def detect_language(text: str) -> str:\n",
        "    \"\"\"More reliable language detection using langdetect\"\"\"\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        return LANG_MAPPING.get(lang, 'en')  # Default to English if not in our mapping\n",
        "    except:\n",
        "        return 'en'  # Fallback to English\n",
        "\n",
        "def translate_with_marian(text: str, source_lang: str, target_lang: str) -> Optional[str]:\n",
        "    \"\"\"Translate using MarianMT if the language pair is supported\"\"\"\n",
        "    if source_lang not in SUPPORTED_PAIRS or target_lang not in SUPPORTED_PAIRS[source_lang]:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        model_name = SUPPORTED_PAIRS[source_lang][target_lang]\n",
        "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        model = MarianMTModel.from_pretrained(model_name)\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        outputs = model.generate(**inputs)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"MarianMT error: {e}\")\n",
        "        return None\n",
        "\n",
        "def translate_text(text: str, source_lang: str, target_lang: str) -> str:\n",
        "    \"\"\"Main translation function with proper auto-detection\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to translate\"\n",
        "\n",
        "    # Handle auto-detection\n",
        "    if source_lang == \"auto\":\n",
        "        detected_lang = detect_language(text)\n",
        "        print(f\"Detected language: {detected_lang}\")\n",
        "\n",
        "        # Special case: if target is same as detected, return original\n",
        "        if detected_lang == target_lang:\n",
        "            return \"Source and target languages appear to be the same\"\n",
        "\n",
        "        # Try MarianMT if available\n",
        "        if detected_lang in SUPPORTED_PAIRS and target_lang in SUPPORTED_PAIRS[detected_lang]:\n",
        "            translation = translate_with_marian(text, detected_lang, target_lang)\n",
        "            if translation:\n",
        "                return f\"Detected {detected_lang.upper()} → 🤖 MarianMT:\\n{translation}\"\n",
        "\n",
        "        # Fallback to online translation\n",
        "        try:\n",
        "            translation = ts.translate_text(text, to_language=target_lang)\n",
        "            return f\"Detected {detected_lang.upper()} → 🌐 Online:\\n{translation}\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Translation failed: {str(e)}\"\n",
        "\n",
        "    # Manual language selection\n",
        "    marian_translation = translate_with_marian(text, source_lang, target_lang)\n",
        "    if marian_translation:\n",
        "        return f\"🤖 MarianMT Translation:\\n{marian_translation}\"\n",
        "\n",
        "    # Fallback to online translation\n",
        "    try:\n",
        "        translation = ts.translate_text(text, to_language=target_lang)\n",
        "        return f\"🌐 Online Translation:\\n{translation}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Translation failed: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=translate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Text\", placeholder=\"Enter text to translate...\"),\n",
        "        gr.Dropdown(\n",
        "            [\"auto\", \"en\", \"es\", \"fr\", \"de\", \"hi\", \"ru\", \"zh\", \"ja\", \"ar\"],\n",
        "            label=\"Source Language\",\n",
        "            value=\"auto\"\n",
        "        ),\n",
        "        gr.Dropdown(\n",
        "            [\"en\", \"es\", \"fr\", \"de\", \"hi\", \"ru\", \"zh\", \"ja\", \"ar\"],\n",
        "            label=\"Target Language\",\n",
        "            value=\"en\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Translation Result\", lines=5),\n",
        "    title=\"🌍 Universal Translator (Working Auto-Detect)\",\n",
        "    description=\"Accurate language auto-detection with MarianMT and online fallback\",\n",
        "    examples=[\n",
        "        [\"Hello world\", \"auto\", \"es\"],\n",
        "        [\"Bonjour le monde\", \"auto\", \"en\"],\n",
        "        [\"自动检测语言\", \"auto\", \"fr\"],\n",
        "        [\"Привет мир\", \"auto\", \"de\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()"
      ]
    }
  ]
}